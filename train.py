"""
å¦å…‹åŠ¨è¡ RL æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨ Stable Baselines3 çš„ PPO ç®—æ³•è®­ç»ƒç©å®¶å¦å…‹å¯¹æŠ— AI Bot
"""

import gymnasium as gym
from stable_baselines3 import PPO, DQN
from environment import TankTroubleEnv  # ä»æ¨¡å—åŒ–çš„ environment.py å¯¼å…¥
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback, CallbackList
from stable_baselines3.common.monitor import Monitor
import os
from datetime import datetime


class RewardLoggerCallback(BaseCallback):
    """
    è‡ªå®šä¹‰å›è°ƒï¼Œç”¨äºåœ¨æ§åˆ¶å°æ‰“å°æ¯ä¸ªå›åˆçš„å¥–åŠ±å’Œç»“æœï¼Œå¹¶è®°å½•åˆ° TensorBoard
    """
    def __init__(self, verbose=0):
        super(RewardLoggerCallback, self).__init__(verbose)
        self.episode_count = 0
        self.win_count = 0

    def _on_step(self) -> bool:
        # æ£€æŸ¥ infos ä¸­æ˜¯å¦æœ‰ episode ä¿¡æ¯ï¼ˆç”± Monitor åŒ…è£…å™¨æä¾›ï¼‰
        for info in self.locals.get("infos", []):
            if "episode" in info:
                self.episode_count += 1
                reward = info["episode"]["r"]
                length = info["episode"]["l"]
                # ä»ç¯å¢ƒè¿”å›çš„ info ä¸­è·å–è‡ªå®šä¹‰ç»“æœ
                result = info.get("result", "N/A")
                
                # è®°å½•åˆ° TensorBoard
                self.logger.record("custom/episode_reward", reward)
                self.logger.record("custom/episode_length", length)
                
                result_emoji = "ğŸ"
                if result == "win":
                    result_emoji = "ğŸ¯ èƒœåˆ©"
                    self.win_count += 1
                    self.logger.record("custom/is_win", 1)
                elif result == "lose":
                    result_emoji = "ğŸ’€ å¤±è´¥"
                    self.logger.record("custom/is_win", 0)
                elif result == "timeout":
                    result_emoji = "â° è¶…æ—¶"
                    self.logger.record("custom/is_win", 0)
                
                # è®¡ç®—èƒœç‡å¹¶è®°å½•
                win_rate = self.win_count / self.episode_count
                self.logger.record("custom/win_rate", win_rate)
                
                # å¼ºåˆ¶å°†è®°å½•å†™å…¥ TensorBoard (åœ¨ rollout ç»“æŸæ—¶ä¼šè‡ªåŠ¨å†™å…¥ï¼Œä½†è¿™é‡Œå¯ä»¥æ‰‹åŠ¨è§¦å‘æˆ–ç­‰å¾…)
                # self.logger.dump(self.num_timesteps)
                
                print(f"  [å›åˆ {self.episode_count}] {result_emoji} | å¥–åŠ±: {reward:7.2f} | æ­¥æ•°: {length} | èƒœç‡: {win_rate:.1%}")
        return True


def train_curriculum(stage_steps=None, algorithm="ppo"):
    """
    è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå‡½æ•° - åˆ†é˜¶æ®µé€æ­¥æå‡éš¾åº¦
    
    é˜¶æ®µ1: æ— å¢™ä½“ï¼ŒBotä¸ç§»åŠ¨ä¸æ”»å‡» (å­¦ä¹ åŸºç¡€æ“ä½œå’Œå°„å‡»)
    é˜¶æ®µ2: æœ‰å¢™ä½“ï¼ŒBotç§»åŠ¨ä½†ä¸æ”»å‡» (å­¦ä¹ å¯¼èˆªå’Œè¿½è¸ªç§»åŠ¨ç›®æ ‡)
    é˜¶æ®µ3: æœ‰å¢™ä½“ï¼ŒBotå®Œæ•´è¡Œä¸º (å­¦ä¹ å®Œæ•´å¯¹æˆ˜)
    
    Args:
        stage_steps: æ¯ä¸ªé˜¶æ®µçš„è®­ç»ƒæ­¥æ•°åˆ—è¡¨ [é˜¶æ®µ1, é˜¶æ®µ2, é˜¶æ®µ3]
        algorithm: è®­ç»ƒç®—æ³•ï¼Œæ”¯æŒ "ppo" æˆ– "dqn"
    """
    if stage_steps is None:
        stage_steps = [400000, 600000, 1000000]  # å¢åŠ è®­ç»ƒæ­¥æ•°
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"./logs/{algorithm}_curriculum_{timestamp}"
    os.makedirs(log_dir, exist_ok=True)
    
    stages = [
        {"difficulty": 1, "name": "é˜¶æ®µ1: é™æ€ç›®æ ‡", "desc": "æ— å¢™ä½“ï¼ŒBoté™æ­¢"},
        {"difficulty": 2, "name": "é˜¶æ®µ2: ç§»åŠ¨ç›®æ ‡", "desc": "æœ‰å¢™ä½“ï¼ŒBotåªç§»åŠ¨"},
        {"difficulty": 3, "name": "é˜¶æ®µ3: å®Œæ•´å¯¹æˆ˜", "desc": "æœ‰å¢™ä½“ï¼ŒBotå®Œæ•´AI"},
    ]
    
    model = None
    
    for i, stage in enumerate(stages):
        print("\n" + "="*60)
        print(f"ğŸ¯ {stage['name']} - {stage['desc']}")
        print(f"   è®­ç»ƒæ­¥æ•°: {stage_steps[i]:,}")
        print("="*60)
        
        # åˆ›å»ºå¯¹åº”éš¾åº¦çš„ç¯å¢ƒ
        stage_log_dir = f"{log_dir}/stage{i+1}"
        os.makedirs(stage_log_dir, exist_ok=True)
        
        env = Monitor(
            TankTroubleEnv(render_mode=None, difficulty=stage["difficulty"]),
            stage_log_dir
        )
        
        if model is None:
            # ç¬¬ä¸€é˜¶æ®µï¼šåˆ›å»ºæ–°æ¨¡å‹
            if algorithm.lower() == "ppo":
                model = PPO(
                    "MlpPolicy",
                    env,
                    verbose=1,
                    learning_rate=0.0001,  # é™ä½å­¦ä¹ ç‡ä»¥æé«˜ç¨³å®šæ€§
                    n_steps=4096,          # å¢åŠ é‡‡æ ·æ­¥æ•°
                    batch_size=256,        # ä¿æŒè¾ƒå¤§çš„ batch_size
                    n_epochs=10,
                    gamma=0.99,
                    gae_lambda=0.95,
                    clip_range=0.1,        # å‡å°è£å‰ªèŒƒå›´ï¼Œä½¿æ›´æ–°æ›´å¹³æ»‘
                    ent_coef=0.01,         # ä¿æŒæ¢ç´¢
                    vf_coef=0.5,           # ä»·å€¼å‡½æ•°æƒé‡
                    max_grad_norm=0.5,     # æ¢¯åº¦è£å‰ª
                    tensorboard_log=log_dir
                )
            elif algorithm.lower() == "dqn":
                model = DQN(
                    "MlpPolicy",
                    env,
                    verbose=1,
                    learning_rate=0.0001,
                    buffer_size=100000,
                    learning_starts=10000,
                    batch_size=256,
                    gamma=0.99,
                    target_update_interval=1000,
                    exploration_fraction=0.3,
                    exploration_initial_eps=1.0,
                    exploration_final_eps=0.05,
                    tensorboard_log=log_dir
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}ã€‚è¯·é€‰æ‹© 'ppo' æˆ– 'dqn'")
        else:
            # åç»­é˜¶æ®µï¼šå¤ç”¨æ¨¡å‹ï¼Œæ›´æ–°ç¯å¢ƒ
            model.set_env(env)
        
        # æ£€æŸ¥ç‚¹å›è°ƒ
        checkpoint_callback = CheckpointCallback(
            save_freq=50000,
            save_path=stage_log_dir,
            name_prefix=f"stage{i+1}_model"
        )
        
        # å¥–åŠ±æ—¥å¿—å›è°ƒ
        reward_logger = RewardLoggerCallback()
        
        # ç»„åˆå›è°ƒ
        callbacks = CallbackList([checkpoint_callback, reward_logger])
        
        # è®­ç»ƒ
        model.learn(
            total_timesteps=stage_steps[i],
            callback=callbacks,
            reset_num_timesteps=False,  # ä¿æŒæ€»æ­¥æ•°è®¡æ•°
            tb_log_name=f"stage{i+1}"
        )
        
        # ä¿å­˜é˜¶æ®µæ¨¡å‹
        model.save(f"{stage_log_dir}/stage{i+1}_final")
        print(f"âœ“ {stage['name']} å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜")
        
        env.close()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(f"{log_dir}/tank_{algorithm}_curriculum_final")
    print(f"\nğŸ‰ è¯¾ç¨‹å­¦ä¹ å®Œæˆï¼æœ€ç»ˆæ¨¡å‹: {log_dir}/tank_{algorithm}_curriculum_final.zip")
    print(f"ğŸ“Š TensorBoard: tensorboard --logdir {log_dir}")


def train_with_checkpoint(total_timesteps=500000, checkpoint_freq=20000, difficulty=3):
    """
    å¸¦æ£€æŸ¥ç‚¹ä¿å­˜çš„è®­ç»ƒå‡½æ•°
    
    Args:
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        checkpoint_freq: æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"./logs/run_{timestamp}"
    os.makedirs(log_dir, exist_ok=True)
    
    # ä½¿ç”¨ Monitor åŒ…è£…ç¯å¢ƒä»¥è®°å½• episode ç»Ÿè®¡
    env = Monitor(TankTroubleEnv(render_mode=None), log_dir)
    
    model = PPO(
        "MlpPolicy", 
        env, 
        verbose=1, 
        learning_rate=0.0001,
        n_steps=4096,
        batch_size=256,
        clip_range=0.1,
        tensorboard_log=log_dir  # å¯ç”¨ TensorBoard æ—¥å¿—
    )

    # æ¯ checkpoint_freq æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    checkpoint_callback = CheckpointCallback(
        save_freq=checkpoint_freq,
        save_path=log_dir,
        name_prefix="tank_model"
    )
    
    # å¥–åŠ±æ—¥å¿—å›è°ƒ
    reward_logger = RewardLoggerCallback()
    
    # ç»„åˆå›è°ƒ
    callbacks = CallbackList([checkpoint_callback, reward_logger])

    print(f"å¼€å§‹è®­ç»ƒ... æ€»æ­¥æ•°: {total_timesteps}")
    print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"ğŸ“Š è¿è¡Œ `tensorboard --logdir {log_dir}` æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
    
    # æ·»åŠ  callback å‚æ•°
    model.learn(total_timesteps=total_timesteps, callback=callbacks)
    
    # æœ€åä¿å­˜æœ€ç»ˆç‰ˆ
    model.save(f"{log_dir}/tank_model_final")
    print(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {log_dir}/tank_model_final.zip")
    env.close()

def train(total_timesteps=3000000, algorithm="ppo", pretrained_model=None):
    """
    åŸºç¡€è®­ç»ƒå‡½æ•°ï¼ˆå¸¦ TensorBoard æ—¥å¿—ï¼‰
    
    Args:
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°ï¼Œå»ºè®®è‡³å°‘ 100,000ï¼Œå¼ºåŠ›æ•ˆæœå¯èƒ½éœ€è¦ 1,000,000+
        algorithm: è®­ç»ƒç®—æ³•ï¼Œæ”¯æŒ "ppo" æˆ– "dqn"
        pretrained_model: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆç”¨äºå¾®è°ƒï¼‰ï¼Œä¸éœ€è¦ .zip åç¼€
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"./logs/{algorithm}_run_{timestamp}"
    os.makedirs(log_dir, exist_ok=True)
    
    # 1. åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼Œä½¿ç”¨ Monitor åŒ…è£…ä»¥è®°å½• episode ç»Ÿè®¡
    print("æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...")
    env = Monitor(TankTroubleEnv(render_mode=None), log_dir)

    # 2. å®šä¹‰æˆ–åŠ è½½æ¨¡å‹
    if pretrained_model:
        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
        print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrained_model}...")
        if not os.path.exists(f"{pretrained_model}.zip"):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {pretrained_model}.zip")
            env.close()
            return
        
        if algorithm.lower() == "ppo":
            model = PPO.load(pretrained_model, env=env, tensorboard_log=log_dir)
        elif algorithm.lower() == "dqn":
            model = DQN.load(pretrained_model, env=env, tensorboard_log=log_dir)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}ã€‚è¯·é€‰æ‹© 'ppo' æˆ– 'dqn'")
        
        print(f"âœ“ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå°†åœ¨æ­¤åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ")
    else:
        # åˆ›å»ºæ–°æ¨¡å‹
        print(f"æ­£åœ¨åˆ›å»º {algorithm.upper()} æ¨¡å‹...")
        print(f"æ­£åœ¨åˆ›å»º {algorithm.upper()} æ¨¡å‹...")
        
        if algorithm.lower() == "ppo":
            model = PPO(
                "MlpPolicy",
                env,
                verbose=1,
                learning_rate=0.0003,
                n_steps=4096, 
                batch_size=256,
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.1,
                tensorboard_log=log_dir
            )
        elif algorithm.lower() == "dqn":
            model = DQN(
                "MlpPolicy",
                env,
                verbose=1,
                learning_rate=0.0003,
                buffer_size=100000,      # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
                learning_starts=10000,   # å¼€å§‹è®­ç»ƒå‰çš„éšæœºæ¢ç´¢æ­¥æ•°
                batch_size=256,
                gamma=0.99,
                target_update_interval=1000,  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
                exploration_fraction=0.3,     # æ¢ç´¢è¡°å‡å æ€»æ­¥æ•°çš„æ¯”ä¾‹
                exploration_initial_eps=1.0,  # åˆå§‹æ¢ç´¢ç‡
                exploration_final_eps=0.05,   # æœ€ç»ˆæ¢ç´¢ç‡
                tensorboard_log=log_dir
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}ã€‚è¯·é€‰æ‹© 'ppo' æˆ– 'dqn'")

    print(f"å¼€å§‹è®­ç»ƒ... æ€»æ­¥æ•°: {total_timesteps}")
    print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"ğŸ“Š è¿è¡Œ `tensorboard --logdir {log_dir}` æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
    print("="*60)
    
    # 3. å¼€å§‹å­¦ä¹ 
    reward_logger = RewardLoggerCallback()
    model.learn(total_timesteps=total_timesteps, callback=reward_logger)

    # 4. ä¿å­˜æ¨¡å‹
    save_path = f"{log_dir}/tank_{algorithm}_model"
    model.save(save_path)
    print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}.zip")
    
    env.close()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¦å…‹å¤§æˆ˜ PPO è®­ç»ƒ")
    parser.add_argument(
        "--mode",
        choices=["basic", "checkpoint", "curriculum"],
        default="basic",
        help="è®­ç»ƒæ¨¡å¼: basic=åŸºç¡€è®­ç»ƒ, checkpoint=å¸¦æ£€æŸ¥ç‚¹ä¿å­˜, curriculum=è¯¾ç¨‹å­¦ä¹ "
    )
    parser.add_argument(
        "--steps",
        type=int,
        default=2000000,
        help="æ€»è®­ç»ƒæ­¥æ•° (é»˜è®¤: 2000000)"
    )
    parser.add_argument(
        "--checkpoint-freq",
        type=int,
        default=20000,
        help="æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ (é»˜è®¤: 20000)"
    )
    parser.add_argument(
        "--stage-steps",
        type=str,
        default="200000,300000,500000",
        help="è¯¾ç¨‹å­¦ä¹ å„é˜¶æ®µæ­¥æ•°ï¼Œé€—å·åˆ†éš” (é»˜è®¤: 200000,300000,500000)"
    )
    parser.add_argument(
        "--algorithm",
        choices=["ppo", "dqn"],
        default="ppo",
        help="è®­ç»ƒç®—æ³•: ppo=Proximal Policy Optimization, dqn=Deep Q-Network (é»˜è®¤: ppo)"
    )
    parser.add_argument(
        "--pretrained-model",
        type=str,
        default=None,
        help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆç”¨äºå¾®è°ƒï¼‰ï¼Œä¸éœ€è¦ .zip åç¼€ã€‚ä¾‹å¦‚: ./logs/run_xxx/tank_ppo_model"
    )
    
    args = parser.parse_args()
    
    print("="*60)
    print("å¦å…‹å¤§æˆ˜ RL è®­ç»ƒ")
    print("="*60)
    
    if args.mode == "basic":
        print(f"æ¨¡å¼: åŸºç¡€è®­ç»ƒ ({args.steps} æ­¥)")
        print(f"ç®—æ³•: {args.algorithm.upper()}")
        if args.pretrained_model:
            print(f"ä»é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ: {args.pretrained_model}")
        train(total_timesteps=args.steps, algorithm=args.algorithm, pretrained_model=args.pretrained_model)
    elif args.mode == "checkpoint":
        print(f"æ¨¡å¼: æ£€æŸ¥ç‚¹è®­ç»ƒ ({args.steps} æ­¥, æ¯ {args.checkpoint_freq} æ­¥ä¿å­˜)")
        train_with_checkpoint(
            total_timesteps=args.steps,
            checkpoint_freq=args.checkpoint_freq
        )
    else:  # curriculum
        stage_steps = [int(s) for s in args.stage_steps.split(",")]
        total = sum(stage_steps)
        print(f"æ¨¡å¼: è¯¾ç¨‹å­¦ä¹  (æ€»æ­¥æ•°: {total:,})")
        print(f"ç®—æ³•: {args.algorithm.upper()}")
        print(f"  é˜¶æ®µ1 (é™æ€ç›®æ ‡): {stage_steps[0]:,} æ­¥")
        print(f"  é˜¶æ®µ2 (ç§»åŠ¨ç›®æ ‡): {stage_steps[1]:,} æ­¥")
        print(f"  é˜¶æ®µ3 (å®Œæ•´å¯¹æˆ˜): {stage_steps[2]:,} æ­¥")
        train_curriculum(stage_steps=stage_steps, algorithm=args.algorithm)
    
    print("="*60)
    print("è®­ç»ƒå®Œæˆ!")
    print("="*60)