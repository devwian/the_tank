"""
å¦å…‹åŠ¨è¡ RL æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨ Stable Baselines3 çš„ PPO ç®—æ³•è®­ç»ƒç©å®¶å¦å…‹å¯¹æŠ— AI Bot
"""

import gymnasium as gym
from stable_baselines3 import PPO
from environment import TankTroubleEnv  # ä»æ¨¡å—åŒ–çš„ environment.py å¯¼å…¥
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback, CallbackList
from stable_baselines3.common.monitor import Monitor
import os
from datetime import datetime


class RewardLoggerCallback(BaseCallback):
    """
    è‡ªå®šä¹‰å›è°ƒï¼Œç”¨äºåœ¨æ§åˆ¶å°æ‰“å°æ¯ä¸ªå›åˆçš„å¥–åŠ±å’Œç»“æœï¼Œå¹¶è®°å½•åˆ° TensorBoard
    """
    def __init__(self, verbose=0):
        super(RewardLoggerCallback, self).__init__(verbose)
        self.episode_count = 0
        self.win_count = 0

    def _on_step(self) -> bool:
        # æ£€æŸ¥ infos ä¸­æ˜¯å¦æœ‰ episode ä¿¡æ¯ï¼ˆç”± Monitor åŒ…è£…å™¨æä¾›ï¼‰
        for info in self.locals.get("infos", []):
            if "episode" in info:
                self.episode_count += 1
                reward = info["episode"]["r"]
                length = info["episode"]["l"]
                # ä»ç¯å¢ƒè¿”å›çš„ info ä¸­è·å–è‡ªå®šä¹‰ç»“æœ
                result = info.get("result", "N/A")
                
                # è®°å½•åˆ° TensorBoard
                self.logger.record("custom/episode_reward", reward)
                self.logger.record("custom/episode_length", length)
                
                result_emoji = "ğŸ"
                if result == "win":
                    result_emoji = "ğŸ¯ èƒœåˆ©"
                    self.win_count += 1
                    self.logger.record("custom/is_win", 1)
                elif result == "lose":
                    result_emoji = "ğŸ’€ å¤±è´¥"
                    self.logger.record("custom/is_win", 0)
                elif result == "timeout":
                    result_emoji = "â° è¶…æ—¶"
                    self.logger.record("custom/is_win", 0)
                
                # è®¡ç®—èƒœç‡å¹¶è®°å½•
                win_rate = self.win_count / self.episode_count
                self.logger.record("custom/win_rate", win_rate)
                
                # å¼ºåˆ¶å°†è®°å½•å†™å…¥ TensorBoard (åœ¨ rollout ç»“æŸæ—¶ä¼šè‡ªåŠ¨å†™å…¥ï¼Œä½†è¿™é‡Œå¯ä»¥æ‰‹åŠ¨è§¦å‘æˆ–ç­‰å¾…)
                # self.logger.dump(self.num_timesteps)
                
                print(f"  [å›åˆ {self.episode_count}] {result_emoji} | å¥–åŠ±: {reward:7.2f} | æ­¥æ•°: {length} | èƒœç‡: {win_rate:.1%}")
        return True


def train_curriculum(stage_steps=None):
    """
    è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå‡½æ•° - åˆ†é˜¶æ®µé€æ­¥æå‡éš¾åº¦
    
    é˜¶æ®µ1: æ— å¢™ä½“ï¼ŒBotä¸ç§»åŠ¨ä¸æ”»å‡» (å­¦ä¹ åŸºç¡€æ“ä½œå’Œå°„å‡»)
    é˜¶æ®µ2: æœ‰å¢™ä½“ï¼ŒBotç§»åŠ¨ä½†ä¸æ”»å‡» (å­¦ä¹ å¯¼èˆªå’Œè¿½è¸ªç§»åŠ¨ç›®æ ‡)
    é˜¶æ®µ3: æœ‰å¢™ä½“ï¼ŒBotå®Œæ•´è¡Œä¸º (å­¦ä¹ å®Œæ•´å¯¹æˆ˜)
    
    Args:
        stage_steps: æ¯ä¸ªé˜¶æ®µçš„è®­ç»ƒæ­¥æ•°åˆ—è¡¨ [é˜¶æ®µ1, é˜¶æ®µ2, é˜¶æ®µ3]
    """
    if stage_steps is None:
        stage_steps = [400000, 600000, 1000000]  # å¢åŠ è®­ç»ƒæ­¥æ•°
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"./logs/curriculum_{timestamp}"
    os.makedirs(log_dir, exist_ok=True)
    
    stages = [
        {"difficulty": 1, "name": "é˜¶æ®µ1: é™æ€ç›®æ ‡", "desc": "æ— å¢™ä½“ï¼ŒBoté™æ­¢"},
        {"difficulty": 2, "name": "é˜¶æ®µ2: ç§»åŠ¨ç›®æ ‡", "desc": "æœ‰å¢™ä½“ï¼ŒBotåªç§»åŠ¨"},
        {"difficulty": 3, "name": "é˜¶æ®µ3: å®Œæ•´å¯¹æˆ˜", "desc": "æœ‰å¢™ä½“ï¼ŒBotå®Œæ•´AI"},
    ]
    
    model = None
    
    for i, stage in enumerate(stages):
        print("\n" + "="*60)
        print(f"ğŸ¯ {stage['name']} - {stage['desc']}")
        print(f"   è®­ç»ƒæ­¥æ•°: {stage_steps[i]:,}")
        print("="*60)
        
        # åˆ›å»ºå¯¹åº”éš¾åº¦çš„ç¯å¢ƒ
        stage_log_dir = f"{log_dir}/stage{i+1}"
        os.makedirs(stage_log_dir, exist_ok=True)
        
        env = Monitor(
            TankTroubleEnv(render_mode=None, difficulty=stage["difficulty"]),
            stage_log_dir
        )
        
        if model is None:
            # ç¬¬ä¸€é˜¶æ®µï¼šåˆ›å»ºæ–°æ¨¡å‹
            model = PPO(
                "MlpPolicy",
                env,
                verbose=1,
                learning_rate=0.0002,  # ç¨å¾®é™ä½å­¦ä¹ ç‡ä»¥æé«˜ç¨³å®šæ€§
                n_steps=2048,
                batch_size=256,        # å¢å¤§ batch_size æé«˜æ¢¯åº¦ä¼°è®¡ç¨³å®šæ€§
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.01,         # å¢åŠ ç†µç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢
                tensorboard_log=log_dir
            )
        else:
            # åç»­é˜¶æ®µï¼šå¤ç”¨æ¨¡å‹ï¼Œæ›´æ–°ç¯å¢ƒ
            model.set_env(env)
        
        # æ£€æŸ¥ç‚¹å›è°ƒ
        checkpoint_callback = CheckpointCallback(
            save_freq=50000,
            save_path=stage_log_dir,
            name_prefix=f"stage{i+1}_model"
        )
        
        # å¥–åŠ±æ—¥å¿—å›è°ƒ
        reward_logger = RewardLoggerCallback()
        
        # ç»„åˆå›è°ƒ
        callbacks = CallbackList([checkpoint_callback, reward_logger])
        
        # è®­ç»ƒ
        model.learn(
            total_timesteps=stage_steps[i],
            callback=callbacks,
            reset_num_timesteps=False,  # ä¿æŒæ€»æ­¥æ•°è®¡æ•°
            tb_log_name=f"stage{i+1}"
        )
        
        # ä¿å­˜é˜¶æ®µæ¨¡å‹
        model.save(f"{stage_log_dir}/stage{i+1}_final")
        print(f"âœ“ {stage['name']} å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜")
        
        env.close()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(f"{log_dir}/tank_curriculum_final")
    print(f"\nğŸ‰ è¯¾ç¨‹å­¦ä¹ å®Œæˆï¼æœ€ç»ˆæ¨¡å‹: {log_dir}/tank_curriculum_final.zip")
    print(f"ğŸ“Š TensorBoard: tensorboard --logdir {log_dir}")


def train_with_checkpoint(total_timesteps=500000, checkpoint_freq=20000, difficulty=3):
    """
    å¸¦æ£€æŸ¥ç‚¹ä¿å­˜çš„è®­ç»ƒå‡½æ•°
    
    Args:
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        checkpoint_freq: æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"./logs/run_{timestamp}"
    os.makedirs(log_dir, exist_ok=True)
    
    # ä½¿ç”¨ Monitor åŒ…è£…ç¯å¢ƒä»¥è®°å½• episode ç»Ÿè®¡
    env = Monitor(TankTroubleEnv(render_mode=None), log_dir)
    
    model = PPO(
        "MlpPolicy", 
        env, 
        verbose=1, 
        learning_rate=0.0003,
        tensorboard_log=log_dir  # å¯ç”¨ TensorBoard æ—¥å¿—
    )

    # æ¯ checkpoint_freq æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    checkpoint_callback = CheckpointCallback(
        save_freq=checkpoint_freq,
        save_path=log_dir,
        name_prefix="tank_model"
    )
    
    # å¥–åŠ±æ—¥å¿—å›è°ƒ
    reward_logger = RewardLoggerCallback()
    
    # ç»„åˆå›è°ƒ
    callbacks = CallbackList([checkpoint_callback, reward_logger])

    print(f"å¼€å§‹è®­ç»ƒ... æ€»æ­¥æ•°: {total_timesteps}")
    print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"ğŸ“Š è¿è¡Œ `tensorboard --logdir {log_dir}` æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
    
    # æ·»åŠ  callback å‚æ•°
    model.learn(total_timesteps=total_timesteps, callback=callbacks)
    
    # æœ€åä¿å­˜æœ€ç»ˆç‰ˆ
    model.save(f"{log_dir}/tank_model_final")
    print(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {log_dir}/tank_model_final.zip")
    env.close()

def train(total_timesteps=3000000):
    """
    åŸºç¡€è®­ç»ƒå‡½æ•°ï¼ˆå¸¦ TensorBoard æ—¥å¿—ï¼‰
    
    Args:
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°ï¼Œå»ºè®®è‡³å°‘ 100,000ï¼Œå¼ºåŠ›æ•ˆæœå¯èƒ½éœ€è¦ 1,000,000+
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"./logs/run_{timestamp}"
    os.makedirs(log_dir, exist_ok=True)
    
    # 1. åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼Œä½¿ç”¨ Monitor åŒ…è£…ä»¥è®°å½• episode ç»Ÿè®¡
    print("æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...")
    env = Monitor(TankTroubleEnv(render_mode=None), log_dir)

    # 2. å®šä¹‰æ¨¡å‹
    print("æ­£åœ¨åˆ›å»º PPO æ¨¡å‹...")
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=0.0003,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        tensorboard_log=log_dir  # å¯ç”¨ TensorBoard æ—¥å¿—
    )

    print(f"å¼€å§‹è®­ç»ƒ... æ€»æ­¥æ•°: {total_timesteps}")
    print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"ğŸ“Š è¿è¡Œ `tensorboard --logdir {log_dir}` æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
    print("="*60)
    
    # 3. å¼€å§‹å­¦ä¹ 
    reward_logger = RewardLoggerCallback()
    model.learn(total_timesteps=total_timesteps, callback=reward_logger)

    # 4. ä¿å­˜æ¨¡å‹
    save_path = f"{log_dir}/tank_ppo_model"
    model.save(save_path)
    print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}.zip")
    
    env.close()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¦å…‹å¤§æˆ˜ PPO è®­ç»ƒ")
    parser.add_argument(
        "--mode",
        choices=["basic", "checkpoint", "curriculum"],
        default="basic",
        help="è®­ç»ƒæ¨¡å¼: basic=åŸºç¡€è®­ç»ƒ, checkpoint=å¸¦æ£€æŸ¥ç‚¹ä¿å­˜, curriculum=è¯¾ç¨‹å­¦ä¹ "
    )
    parser.add_argument(
        "--steps",
        type=int,
        default=2000000,
        help="æ€»è®­ç»ƒæ­¥æ•° (é»˜è®¤: 1000000)"
    )
    parser.add_argument(
        "--checkpoint-freq",
        type=int,
        default=20000,
        help="æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ (é»˜è®¤: 20000)"
    )
    parser.add_argument(
        "--stage-steps",
        type=str,
        default="200000,300000,500000",
        help="è¯¾ç¨‹å­¦ä¹ å„é˜¶æ®µæ­¥æ•°ï¼Œé€—å·åˆ†éš” (é»˜è®¤: 200000,300000,500000)"
    )
    
    args = parser.parse_args()
    
    print("="*60)
    print("å¦å…‹å¤§æˆ˜ RL è®­ç»ƒ")
    print("="*60)
    
    # if args.mode == "basic":
    #     print(f"æ¨¡å¼: åŸºç¡€è®­ç»ƒ ({args.steps} æ­¥)")
    #     train(total_timesteps=args.steps)
    # elif args.mode == "checkpoint":
    #     print(f"æ¨¡å¼: æ£€æŸ¥ç‚¹è®­ç»ƒ ({args.steps} æ­¥, æ¯ {args.checkpoint_freq} æ­¥ä¿å­˜)")
    #     train_with_checkpoint(
    #         total_timesteps=args.steps,
    #         checkpoint_freq=args.checkpoint_freq
    #     )
    # else:  # curriculum
    #     stage_steps = [int(s) for s in args.stage_steps.split(",")]
    #     total = sum(stage_steps)
    #     print(f"æ¨¡å¼: è¯¾ç¨‹å­¦ä¹  (æ€»æ­¥æ•°: {total:,})")
    #     print(f"  é˜¶æ®µ1 (é™æ€ç›®æ ‡): {stage_steps[0]:,} æ­¥")
    #     print(f"  é˜¶æ®µ2 (ç§»åŠ¨ç›®æ ‡): {stage_steps[1]:,} æ­¥")
    #     print(f"  é˜¶æ®µ3 (å®Œæ•´å¯¹æˆ˜): {stage_steps[2]:,} æ­¥")
    #     train_curriculum(stage_steps=stage_steps)
    print(f"æ¨¡å¼: åŸºç¡€è®­ç»ƒ ({args.steps} æ­¥)")
    train(total_timesteps=args.steps)
    
    print("="*60)
    print("è®­ç»ƒå®Œæˆ!")
    print("="*60)